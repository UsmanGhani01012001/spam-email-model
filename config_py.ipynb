{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# config.py\n",
        "\n",
        "import os\n",
        "\n",
        "# --- Data Configuration ---\n",
        "# IMPORTANT: Change this to your actual file path\n",
        "FILE_PATH = os.path.join(os.path.dirname(__file__), 'mydataset2900.xlsx')\n",
        "TEXT_COLUMN = 'lyrics'\n",
        "# List of your emotional label columns (make sure these match your Excel exact column names)\n",
        "LABEL_COLUMNS = ['hope', 'joy', 'anger', 'obsession', 'lust', 'fear/tension', 'tender', 'sad']\n",
        "VALIDATION_SPLIT = 0.20 # 20% for validation, 80% for training\n",
        "\n",
        "# --- Model Configuration ---\n",
        "# Choose 'BiGRU_Attention' or 'BERT_XLSTM'\n",
        "MODEL_TYPE = 'BiGRU_Attention'\n",
        "\n",
        "# Hyperparameters for BiGRU_Attention Model\n",
        "MAX_WORDS = 15000       # Max unique words for Tokenizer\n",
        "MAX_SEQUENCE_LENGTH = 300 # Max sequence length for padding\n",
        "EMBEDDING_DIM = 400     # Embedding dimension for BiGRU\n",
        "GRU_UNITS = 200         # Units in each GRU layer\n",
        "DENSE_UNITS = 128       # Units in the intermediate Dense layer\n",
        "DROPOUT_RATE = 0.6      # Dropout rate for regularization\n",
        "L2_REG_FACTOR = 0.001   # L2 Regularization factor for BiGRU (Adjust as needed)\n",
        "LEARNING_RATE = 0.001   # Learning rate for Adam optimizer (Default is 0.001)\n",
        "\n",
        "# Hyperparameters for BERT_XLSTM Model\n",
        "BERT_MAX_LEN = 512      # Max sequence length for BERT tokenizer\n",
        "XLSTM_UNITS = 128       # Units in the LSTM layer for BERT model\n",
        "BERT_DENSE_UNITS = 256  # Units in dense layers for BERT model\n",
        "BERT_DROPOUT_RATE = 0.3 # Dropout rate for BERT model\n",
        "BERT_L2_REG_FACTOR = 0.001 # L2 Regularization factor for BERT model (Adjust as needed)\n",
        "\n",
        "\n",
        "# --- Training Configuration ---\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20             # Max epochs (EarlyStopping will typically halt earlier)\n",
        "\n",
        "# --- Callbacks Configuration ---\n",
        "EARLY_STOPPING_PATIENCE = 5 # Number of epochs with no improvement after which training will be stopped.\n",
        "REDUCE_LR_FACTOR = 0.5    # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
        "REDUCE_LR_PATIENCE = 2    # Number of epochs with no improvement after which learning rate will be reduced.\n",
        "MIN_LR = 1e-6             # Lower bound on the learning rate.\n",
        "\n",
        "# --- Model Saving ---\n",
        "# Directory to save the trained model and tokenizer\n",
        "SAVE_DIR = 'saved_models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, f'{MODEL_TYPE}_emotion_model.h5')\n",
        "TOKENIZER_SAVE_PATH = os.path.join(SAVE_DIR, 'tokenizer.json') # Only for BiGRU_Attention model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3dqYn-C1mhmU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}