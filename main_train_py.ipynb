{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# main_train.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Import configurations and helper functions\n",
        "from config import (\n",
        "    FILE_PATH, TEXT_COLUMN, LABEL_COLUMNS, VALIDATION_SPLIT, MODEL_TYPE,\n",
        "    MAX_WORDS, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, GRU_UNITS, DENSE_UNITS,\n",
        "    DROPOUT_RATE, L2_REG_FACTOR, LEARNING_RATE,\n",
        "    BERT_MAX_LEN, XLSTM_UNITS, BERT_DENSE_UNITS, BERT_DROPOUT_RATE, BERT_L2_REG_FACTOR,\n",
        "    BATCH_SIZE, EPOCHS,\n",
        "    EARLY_STOPPING_PATIENCE, REDUCE_LR_FACTOR, REDUCE_LR_PATIENCE, MIN_LR,\n",
        "    SAVE_DIR, MODEL_SAVE_PATH, TOKENIZER_SAVE_PATH\n",
        ")\n",
        "from data_handler import load_and_preprocess_data, tokenize_and_pad_sequences, get_bert_embeddings, split_data, save_tokenizer\n",
        "from model_architectures import build_bigru_attention_model, build_bert_xlstm_model\n",
        "from utils import plot_training_history\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to load data, build, train, and evaluate the selected deep learning model.\n",
        "    \"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        os.makedirs(SAVE_DIR)\n",
        "        print(f\"Created directory: {SAVE_DIR}\")\n",
        "\n",
        "    # --- 1. Data Loading and Preprocessing ---\n",
        "    try:\n",
        "        df = load_and_preprocess_data(FILE_PATH, TEXT_COLUMN, LABEL_COLUMNS)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        print(f\"Critical error during data loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # Prepare labels (convert DataFrame to NumPy array)\n",
        "    y = df[LABEL_COLUMNS].values.astype(np.float32)\n",
        "    print(f\"Labels shape: {y.shape}\")\n",
        "    print(f\"Number of labels: {len(LABEL_COLUMNS)}\")\n",
        "\n",
        "    X = None # Initialize X\n",
        "    tokenizer_for_save = None # Initialize tokenizer for saving\n",
        "\n",
        "    if MODEL_TYPE == 'BiGRU_Attention':\n",
        "        # --- 2. Tokenization and Padding for BiGRU ---\n",
        "        X, tokenizer_for_save = tokenize_and_pad_sequences(\n",
        "            df[TEXT_COLUMN], MAX_WORDS, MAX_SEQUENCE_LENGTH\n",
        "        )\n",
        "    elif MODEL_TYPE == 'BERT_XLSTM':\n",
        "        # --- 2. Generate BERT Embeddings ---\n",
        "        X = get_bert_embeddings(df[TEXT_COLUMN], max_len=BERT_MAX_LEN, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        print(f\"Error: Unknown MODEL_TYPE '{MODEL_TYPE}'. Please choose 'BiGRU_Attention' or 'BERT_XLSTM'.\")\n",
        "        return\n",
        "\n",
        "    if X is None:\n",
        "        print(\"Error: Features (X) could not be generated. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Data Splitting ---\n",
        "    X_train, X_val, y_train, y_val = split_data(X, y, VALIDATION_SPLIT)\n",
        "\n",
        "    # --- 4. Build the Model ---\n",
        "    model = None\n",
        "    if MODEL_TYPE == 'BiGRU_Attention':\n",
        "        model = build_bigru_attention_model(\n",
        "            max_words=MAX_WORDS,\n",
        "            max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            gru_units=GRU_UNITS,\n",
        "            dense_units=DENSE_UNITS,\n",
        "            dropout_rate=DROPOUT_RATE,\n",
        "            num_labels=len(LABEL_COLUMNS),\n",
        "            l2_reg_factor=L2_REG_FACTOR\n",
        "        )\n",
        "    elif MODEL_TYPE == 'BERT_XLSTM':\n",
        "        model = build_bert_xlstm_model(\n",
        "            num_labels=len(LABEL_COLUMNS),\n",
        "            lstm_units=XLSTM_UNITS,\n",
        "            dense_units=BERT_DENSE_UNITS,\n",
        "            dropout_rate=BERT_DROPOUT_RATE,\n",
        "            l2_reg_factor=BERT_L2_REG_FACTOR\n",
        "        )\n",
        "\n",
        "    if model is None:\n",
        "        print(\"Error: Model could not be built. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- 5. Compile the Model ---\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[Precision(thresholds=0.5), Recall(thresholds=0.5)])\n",
        "    print(\"Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # --- 6. Train the Model ---\n",
        "    print(\"\\nTraining the model...\")\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=REDUCE_LR_FACTOR,\n",
        "        patience=REDUCE_LR_PATIENCE,\n",
        "        min_lr=MIN_LR\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "    print(\"\\nModel training complete.\")\n",
        "\n",
        "    # --- 7. Evaluate the Model and Calculate Macro-averaged F1-score ---\n",
        "    print(\"\\nEvaluating the model on validation data...\")\n",
        "    y_pred_proba = model.predict(X_val)\n",
        "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    macro_f1 = f1_score(y_val, y_pred_binary, average='macro')\n",
        "    print(f\"\\nMacro-averaged F1-score on validation set: {macro_f1:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining History Metrics:\")\n",
        "    print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "    if 'precision' in history.history:\n",
        "        print(f\"Final Training Precision (threshold 0.5): {history.history['precision'][-1]:.4f}\")\n",
        "    if 'val_precision' in history.history:\n",
        "        print(f\"Final Validation Precision (threshold 0.5): {history.history['val_precision'][-1]:.4f}\")\n",
        "    if 'recall' in history.history:\n",
        "        print(f\"Final Training Recall (threshold 0.5): {history.history['recall'][-1]:.4f}\")\n",
        "    if 'val_recall' in history.history:\n",
        "        print(f\"Final Validation Recall (threshold 0.5): {history.history['val_recall'][-1]:.4f}\")\n",
        "\n",
        "    # --- 8. Save the Model and Tokenizer ---\n",
        "    print(f\"\\nSaving the model to {MODEL_SAVE_PATH}...\")\n",
        "    model.save(MODEL_SAVE_PATH)\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "    if MODEL_TYPE == 'BiGRU_Attention' and tokenizer_for_save:\n",
        "        save_tokenizer(tokenizer_for_save, TOKENIZER_SAVE_PATH)\n",
        "\n",
        "    # --- 9. Plot Training History ---\n",
        "    plot_training_history(history)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DATjlHrtm3Ou"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}