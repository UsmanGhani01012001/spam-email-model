{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# data_handler.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tqdm import tqdm\n",
        "import json # For saving/loading tokenizer\n",
        "\n",
        "def load_and_preprocess_data(file_path, text_column, label_columns):\n",
        "    \"\"\"\n",
        "    Loads data from an Excel file and performs initial preprocessing.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Excel dataset.\n",
        "        text_column (str): Name of the column containing the text data (lyrics).\n",
        "        label_columns (list): List of column names containing the emotion labels.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Processed DataFrame.\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified file does not exist.\n",
        "        ValueError: If essential columns are missing or data is invalid.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found. Please check the path.\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(\"Data loaded successfully.\")\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(\"Dataset head:\")\n",
        "        print(df.head())\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"An error occurred during data loading: {e}\")\n",
        "\n",
        "    # Drop rows where lyrics are missing or empty\n",
        "    df.dropna(subset=[text_column], inplace=True)\n",
        "    df = df[df[text_column].astype(str).str.strip() != '']\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Error: No valid lyrics found after dropping missing/empty rows.\")\n",
        "\n",
        "    print(\"\\nStarting text preprocessing...\")\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "    print(\"Text cleaning complete.\")\n",
        "\n",
        "    # Ensure label columns exist and are numeric (0 or 1)\n",
        "    print(\"\\nPreparing labels...\")\n",
        "    for col in label_columns:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Error: Label column '{col}' not found in the dataset. Please check LABEL_COLUMNS configuration.\")\n",
        "        # Ensure they are integers (0 or 1)\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "        # Basic check for non-binary values that might indicate an issue\n",
        "        if not df[col].isin([0, 1]).all():\n",
        "            print(f\"Warning: Column '{col}' contains values other than 0 or 1 after conversion. This might affect multi-label binarization.\")\n",
        "    print(\"Labels prepared.\")\n",
        "    return df\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by converting to lowercase, removing punctuation,\n",
        "    and extra whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input string to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned string.\n",
        "    \"\"\"\n",
        "    text = str(text).lower() # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def tokenize_and_pad_sequences(texts, max_words, max_sequence_length, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Tokenizes text data and pads sequences.\n",
        "\n",
        "    Args:\n",
        "        texts (pd.Series): Series of text data.\n",
        "        max_words (int): Maximum number of unique words to keep.\n",
        "        max_sequence_length (int): Max length for padding sequences.\n",
        "        tokenizer (tf.keras.preprocessing.text.Tokenizer, optional): Pre-fitted tokenizer.\n",
        "                                                                     If None, a new one is fitted.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (padded_sequences, tokenizer)\n",
        "    \"\"\"\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(num_words=max_words, oov_token=\"<unk>\")\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "        print(f\"Found {len(tokenizer.word_index)} unique tokens.\")\n",
        "    else:\n",
        "        print(\"Using provided tokenizer.\")\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "def get_bert_embeddings(texts, max_len=512, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generates BERT embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "        texts (pd.Series): Series of text data.\n",
        "        max_len (int): Maximum sequence length for BERT tokenizer.\n",
        "        batch_size (int): Batch size for processing texts for BERT embeddings.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of BERT embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\nInitializing BERT Tokenizer and Model...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    print(\"Generating BERT embeddings...\")\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        tokens = tokenizer(batch.tolist(), padding='max_length', truncation=True,\n",
        "                           max_length=max_len, return_tensors='tf')\n",
        "        outputs = bert_model(tokens)['last_hidden_state'][:, 0, :]  # CLS token\n",
        "        embeddings.append(outputs.numpy())\n",
        "    print(\"BERT embeddings generated successfully.\")\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "def split_data(X, y, validation_split, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits data into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Features (padded sequences or embeddings).\n",
        "        y (numpy.ndarray): Labels.\n",
        "        validation_split (float): Proportion of data to use for validation.\n",
        "        random_state (int): Seed for random splitting.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, X_val, y_train, y_val)\n",
        "    \"\"\"\n",
        "    print(f\"\\nSplitting data into {100*(1-validation_split)}% train and {100*validation_split}% validation...\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=validation_split, random_state=random_state)\n",
        "    print(f\"Train data shape: {X_train.shape}, Train labels shape: {y_train.shape}\")\n",
        "    print(f\"Validation data shape: {X_val.shape}, Validation labels shape: {y_val.shape}\")\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def save_tokenizer(tokenizer, path):\n",
        "    \"\"\"Saves the tokenizer configuration to a JSON file.\"\"\"\n",
        "    tokenizer_json = tokenizer.to_json()\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "    print(f\"Tokenizer saved to {path}\")\n",
        "\n",
        "def load_tokenizer(path):\n",
        "    \"\"\"Loads the tokenizer configuration from a JSON file.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        tokenizer_json = json.load(f)\n",
        "    tokenizer = Tokenizer.from_json(tokenizer_json)\n",
        "    print(f\"Tokenizer loaded from {path}\")\n",
        "    return tokenizer"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8K4wGHgFmqPM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}