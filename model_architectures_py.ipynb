{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# model_architectures.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Bidirectional, Dropout, Layer, Reshape\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# --- Custom Attention Layer (Bahdanau Attention) ---\n",
        "# This layer calculates attention weights and applies them to the GRU outputs.\n",
        "class BahdanauAttention(Layer):\n",
        "    \"\"\"\n",
        "    Custom Bahdanau Attention Layer for Keras models.\n",
        "    Calculates attention weights and applies them to a sequence of values.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(BahdanauAttention, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.W1 = Dense(units, name='attention_W1')\n",
        "        self.W2 = Dense(units, name='attention_W2')\n",
        "        self.V = Dense(1, name='attention_V')\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input_shape[0] is query, input_shape[1] is values\n",
        "        super(BahdanauAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \"\"\"\n",
        "        Performs the forward pass for the attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            query (tf.Tensor): The query tensor (e.g., last hidden state of GRU).\n",
        "                               Shape: (batch_size, gru_units)\n",
        "            values (tf.Tensor): The values tensor (e.g., sequence of hidden states from GRU).\n",
        "                                Shape: (batch_size, max_sequence_length, gru_units)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (context_vector, attention_weights)\n",
        "        \"\"\"\n",
        "        # query shape == (batch_size, gru_units)\n",
        "        # values shape == (batch_size, max_sequence_length, gru_units)\n",
        "\n",
        "        # expand_dims to add time axis to query\n",
        "        # query_with_time_axis shape == (batch_size, 1, gru_units)\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_sequence_length, 1)\n",
        "        # score is a non-normalized probability distribution over the attention weights.\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_sequence_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, gru_units)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(BahdanauAttention, self).get_config()\n",
        "        config.update({'units': self.units})\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def build_bigru_attention_model(max_words, max_sequence_length, embedding_dim,\n",
        "                                gru_units, dense_units, dropout_rate, num_labels, l2_reg_factor):\n",
        "    \"\"\"\n",
        "    Builds a Bidirectional GRU model with a Bahdanau Attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        max_words (int): Maximum number of unique words in the vocabulary.\n",
        "        max_sequence_length (int): Maximum length of input sequences.\n",
        "        embedding_dim (int): Dimension of the word embeddings.\n",
        "        gru_units (int): Number of units in each GRU layer.\n",
        "        dense_units (int): Number of units in the intermediate Dense layer.\n",
        "        dropout_rate (float): Dropout rate for regularization.\n",
        "        num_labels (int): Number of output labels for multi-label classification.\n",
        "        l2_reg_factor (float): L2 regularization factor.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding the BiGRU + Attention model...\")\n",
        "\n",
        "    l2_regularizer = l2(l2_reg_factor)\n",
        "\n",
        "    # Input Layer\n",
        "    input_layer = Input(shape=(max_sequence_length,), name='input_lyrics')\n",
        "\n",
        "    # Embedding Layer\n",
        "    embedding_layer = Embedding(input_dim=max_words,\n",
        "                                output_dim=embedding_dim,\n",
        "                                input_length=max_sequence_length,\n",
        "                                name='word_embedding')(input_layer)\n",
        "    embedding_layer = Dropout(dropout_rate, name='embedding_dropout')(embedding_layer)\n",
        "\n",
        "    # Bidirectional GRU Layers with L2 regularization\n",
        "    bigru_output_1 = Bidirectional(GRU(units=gru_units, return_sequences=True,\n",
        "                                       kernel_regularizer=l2_regularizer,\n",
        "                                       recurrent_regularizer=l2_regularizer),\n",
        "                                   name='bigru_1')(embedding_layer)\n",
        "    bigru_output_1 = Dropout(dropout_rate, name='bigru_dropout_1')(bigru_output_1)\n",
        "\n",
        "    bigru_output_2 = Bidirectional(GRU(units=gru_units, return_sequences=True,\n",
        "                                       kernel_regularizer=l2_regularizer,\n",
        "                                       recurrent_regularizer=l2_regularizer),\n",
        "                                   name='bigru_2')(bigru_output_1)\n",
        "    bigru_output_2 = Dropout(dropout_rate, name='bigru_dropout_2')(bigru_output_2)\n",
        "\n",
        "    bigru_output_3 = Bidirectional(GRU(units=gru_units, return_sequences=True,\n",
        "                                       kernel_regularizer=l2_regularizer,\n",
        "                                       recurrent_regularizer=l2_regularizer),\n",
        "                                   name='bigru_3')(bigru_output_2)\n",
        "    bigru_output_3 = Dropout(dropout_rate, name='bigru_dropout_3')(bigru_output_3)\n",
        "\n",
        "    # Query Generator GRU for Attention\n",
        "    query_generator_bigru = Bidirectional(GRU(units=gru_units, return_sequences=False,\n",
        "                                             kernel_regularizer=l2_regularizer,\n",
        "                                             recurrent_regularizer=l2_regularizer),\n",
        "                                         name='query_generator_gru')(bigru_output_3)\n",
        "\n",
        "    # Attention Mechanism\n",
        "    # Units for BahdanauAttention should match the concatenated output dimension of BiGRU (gru_units * 2)\n",
        "    attention_layer = BahdanauAttention(gru_units * 2, name='bahdanau_attention')\n",
        "    context_vector, attention_weights = attention_layer(query_generator_bigru, bigru_output_3)\n",
        "    context_vector = Dropout(dropout_rate, name='attention_dropout')(context_vector)\n",
        "\n",
        "    # First Dense Layer with L2 regularization\n",
        "    dense_layer_1 = Dense(units=dense_units, activation='relu',\n",
        "                          kernel_regularizer=l2_regularizer, name='dense_1')(context_vector)\n",
        "    dense_layer_1 = Dropout(dropout_rate, name='dense_dropout_1')(dense_layer_1)\n",
        "\n",
        "    # Output Layer\n",
        "    output_layer = Dense(num_labels, activation='sigmoid',\n",
        "                         kernel_regularizer=l2_regularizer, name='output_emotions')(dense_layer_1)\n",
        "\n",
        "    # Create the Model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='BiGRU_Attention_Model')\n",
        "    print(\"BiGRU + Attention model built.\")\n",
        "    return model\n",
        "\n",
        "def build_bert_xlstm_model(num_labels, lstm_units, dense_units, dropout_rate, l2_reg_factor):\n",
        "    \"\"\"\n",
        "    Builds a model that takes BERT embeddings as input and processes them\n",
        "    with a Bidirectional LSTM and multiple Dense layers.\n",
        "\n",
        "    Args:\n",
        "        num_labels (int): Number of output labels.\n",
        "        lstm_units (int): Number of units in the LSTM layer.\n",
        "        dense_units (int): Number of units in each intermediate Dense layer.\n",
        "        dropout_rate (float): Dropout rate for regularization.\n",
        "        l2_reg_factor (float): L2 regularization factor.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding the BERT + XLSTM model...\")\n",
        "    l2_regularizer = l2(l2_reg_factor)\n",
        "\n",
        "    # Input layer for BERT embeddings (768 for bert-base-uncased)\n",
        "    input_layer = Input(shape=(768,), name='bert_embeddings_input')\n",
        "\n",
        "    # Reshape for LSTM: (batch_size, 1, 768)\n",
        "    x = Reshape((1, 768), name='reshape_for_lstm')(input_layer)\n",
        "\n",
        "    # Bidirectional LSTM layer\n",
        "    x = Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=False,\n",
        "                                           kernel_regularizer=l2_regularizer,\n",
        "                                           recurrent_regularizer=l2_regularizer),\n",
        "                      name='bidirectional_lstm')(x)\n",
        "    x = Dropout(dropout_rate, name='lstm_dropout')(x)\n",
        "\n",
        "    # Multiple Dense Layers with L2 Regularization and Dropout\n",
        "    for i in range(5): # Reduced from 10 to 5 for initial stability, can be tuned\n",
        "        x = Dense(dense_units, activation='relu', kernel_regularizer=l2_regularizer,\n",
        "                  name=f'dense_layer_{i+1}')(x)\n",
        "        x = Dropout(dropout_rate, name=f'dense_dropout_{i+1}')(x)\n",
        "\n",
        "    # Output Layer\n",
        "    output_layer = Dense(num_labels, activation='sigmoid',\n",
        "                         kernel_regularizer=l2_regularizer, name='output_emotions')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='BERT_XLSTM_Model')\n",
        "    print(\"BERT + XLSTM model built.\")\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UVNhCRYImwzJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}